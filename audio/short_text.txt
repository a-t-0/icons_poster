Welcome to this presentation on the robustness of spiking neural networks. We will look into leveraging brain-adaptation principles too increase radiation resistance of neuromorphic space hardware. We will cover why it is relevant and how the research is performed. 

To test the hypothesis, a distributed minimum dominating set approximation algorithm is selected. This algorithm is presented by Ali pour. 

The input of this algorithm is a graph consisting of nodes and edges. The purpose of this algorithm is to approximate the minimum dominating set. The latter is the smallest amount of nodes that can reach the entire graph by sending one message to their neighbors (and itself). This algorithm can be described with the following steps. 
First, each node chooses a random number between zero and one. Then each node computes how many neigbors it has. That is called the degree. The weight of this node then becomes the random number plus the degree. Then, each node looks at the weights of its neighbors and adds 1 to the mark of the neighbor with the highest weight. This mark is referred too as: x_i. 

Then a loop starts. The original random number of the node gets added to the mark, yielding a new weight per node. The marks are reset. Then, each node looks at its neighbors again, and adds one to the node with the highest weight. The original random number of the node gets added to the new mark, yielding a new weight per node. This cycle repeats for em rounds.

At the end of the algorithm, the nodes that have a mark, are in the dominating set. And the others are not.


This algorithm has been converted into a spiking neural network. There are 5 different neuron types in this network, and a trivial connecting node. The latter can be ignored. At the start of the algorithm, the spike once neurons fire. Just like the random neurons. The selector neurons spike continuously untill the degree receiver neurons spike. The degree receiver neurons spike once if they reach their threshold. The counter neurons store the amount of spikes that came in. 

One aproximately horizontal line of neurons in figure 2 represent a circuit for a single node. The spike once neurons of node two reach out to the degree receiver neurons representing the neighbors of that node. So for node 2 of the graph in figure 1, those neighbours are: zero, one, and three. For degree receiver nodes, the first index indicates which node it represents. The second index represents which neighbor it represents. The third index represents which round, of m, it represents. So the spike once neurons ensure the degree receivers receive the degree of a node. 

The degree receivers also need to store the random number. So the degree receiver two underscore three gets an incoming random value from the random neuron representing the third node. Just like the degree receiver four underscore 3. This degree and random weight then represent the weight of the neuron. So then the degree receiver neuron with the highest weight, should fire for a given node. To prevent the degree receiver neurons from spiking immediately, they are initiated with a negative input. This negative input is included in the weight from the random neuron into the degree receiver. That is why it is negative. The selector neuron two can no excite the degree receiver neurons belonging to node two. Once the first degree receiver of node two spikes, it inhibits the selector neuron two. Since there is a delay of two from the degree receiver to the selector neurons and back, the random number is multiplied with a delta of at least two. Recall, the random number should never make more than one mark count difference. That is why the synaptic weight of the spike once neuron is multiplied with the maximum random number and delta. The degree receiver neuron outputs a spike to the counter neuron, which stores the input signal as a count.

If the algorithm is ran for multiple rounds, the degree receivers of round em can send their counts directly as input into the degree receivers of the next round.

The adaptation is implemented in the form of redundancy. For example, for each spike once neuron, a redundant spike once neuron is created. These redundant neurons typically receive the same inputs and outputs as the original neuron. If a redundant neuron does not have an input, it will have a delay of one before it spikes. This allows it to be inhibited by its original neuron. The algorithm is currently not adjusted to accomodate this delay. Therefore, some calculations with the delayed neurons may still be faulty.

For most experiment settings the performance with adaptation was equivalent to the one without adaptation. The adaptation provided a slight advantage for simulations in which twentyfive percent of the neurons died. The scope of the experiment needs to be increased to build more confidence in the results.