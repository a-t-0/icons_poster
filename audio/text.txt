Welcome to this presentation on the robustness of spiking neural networks. We will look into leveraging brain-adaptation principles too increase radiation resistance of neuromorphic space hardware. I will explain how I came up with this research, why it is relevant,  what it solves, and how that is done. 

During neuro-science lectures at Leiden University, I learned how brain adaptation can sometimes allow people to partially recover from brain lesions.
As part of a space exploration education, I learned how radiation can damage space hardware and result in faulty operating conditions. And in the cognitive computing specialisation I learned how spiking neural networks can be used to perform energy efficient computations. Putting these three insights together, I wondered if brain adaptation principles could be applied in spiking neural networks too increase the radiation robustness of neuromorphic space hardware.

Space crafts are typically heavily energy constrained. This makes neuromorphic hardware an interesting candidate for space applications. Accordingly, there is a growing interest in launching neuromorphic hardware into space.

To test the hypothesis, a distributed minimum dominating set approximation algorithm is selected. This algorithm is presented by Alipour. 

The input of this algorithm is a graph consisting of nodes and edges. The purpose of this algo-rithm is to approximate the minimum dominating set. The latter is the smallest amount of nodes that can reach the entire graph by sending 1 message to their neighbors (and itself). This algorithm can be described with the following steps. 
First, each node chooses a random number between 0 and 1. Then each node computes how many neigbours it has. That is called the degree. The weight of this node then becomes the random number plus the degree. Then, each node looks at the weights of its neighbors and adds plus one to the mark of the neighbour with the highest weight. This mark is referred too as: x_i. 

Then a loop starts. The original random number of the node gets added to mark, yielding a new weight per node. The marks are reset. Then, each node looks at its neighbors again, and adds 1 to the node with the highest weight. The original random number of the node gets added to the new mark, yielding a new weight per node. This cycle repeats for em rounds.

At the end of the algorithm, the nodes that have a mark, are in the dominating set. And the others are not.


This algorithm is converted into a spiking neural network by creating a neural network that perform the same computations. There are 5 different neuron types in this network, and a trivial connecting node. The latter can be ignored. At the start of the algorithm, the spike once neurons fire. Just like the random neurons. The selector neurons spike continuously untill the degree receiver neurons spike. The degree receiver neurons spike once if they reach their threshold. The counter neurons store the amount of spikes that came in. 

One aproximately horizontal line of neurons represent a circuit for a single node. The spike once neurons of node 2 reach out to the degree receiver neurons representing the neighbors 0, 1, and 3 of node 2. So for degree receiver nodes, the first index indicates which node it represents. The second index represents which neighbour it represents. The third neuron represents which round, of m, it represents. So the spike once neurons ensure the degree receivers receive the degree of a node. 

The degree receivers also need to store the random number. So the degree receiver 2 underscore 3 gets an incoming random value from the random neuron representing node 3. Just like the degree receiver 4 underscore 3. This degree and random weight then represent the weight of the neuron. So then the degree receiver neuron with the highest weight, should fire for a given node. To prevent the degree receiver neurons from spiking immediately, they are initiated with a negative input. This negative input is included in the weight from the random neuron into the degree receiver. That is why it is negative. The selector neuron two can no excite the degree receiver neurons belonging to node two. Once the first degree receiver of node 2 spikes, it inhibits the selector neuron 2. Since there is a delay of 2 from the degree receiver to the selector neurons and back, the random number is multiplied with a delta of at least 2. Recall, the random number should never make more than 1 mark count difference. That is why the synaptic weight of the spike once neuron is multiplied with the maximum random number and delta. The degree receiver neuron outputs a spike to the counter neuron, which stores the input signal as a count.

If the algorithm is ran for multiple rounds, the degree receivers of round em can send their counts directly as input into the degree receivers of the next round.

The adaptation is implemented in the form of redundancy. For example, for each spike once neuron, a redundant spike once neuron is created. These redundant neurons typically receive the same inputs and outputs as the original neuron. If a redundant neuron does not have an input, it will have a delay of 1 before it spikes. This allows it to be inhibited by its original neuron. The algorithm is currently not adjusted to accomodate this delay. Therefore, some calculations with the delayed neurons may still be faulty.

The radiation is simulated by simulating permanent neuron deaths. This neuron death is simulated by setting the spike threshold to 1000 volts. The modular code also facilitates transient radiation effects, as well as radiation induced neuronal- and synaptic property changes. These two options are not tested though.

The experiment was ran on the Lava 0.0.3 framework by Intel. For most experiment settings the performance with and without adaptation was equivalent. The adaptation provided a slight advantage for simulations in which 25 percent of the neurons died. The scope of the experiment needs to be increased to build more confidence in the results. To this end, a custom networkx SNN simulator is built as an alternative backend. This simulator allows for larger graph sizes and eliminates reliability issues.